# Personality-Classifying
The Project of NLP By Xinyu Dai

(本项目已经上传github，[我的仓库](https://github.com/YksYiZov/Personality-Classifying/tree/task-3))
## Task 1
### 子问题1
#### 概述
在这个问题中，我们采用了KNN来进行分类任务，具体的执行思路如下
#### 思路
- 导入文本，进行词数统计工作，分析统计的词数，对数据进行预处理
- 将MBTI人格分类任务分为四个子任务，训练四个KNN分类器，每个KNN完成一个方面的二分类任务
#### 实施细节
##### KNN的训练
- 数据的采集工作我们把全部的训练样本分为两类
- 统计两类样本各个词出现的数量
- 删除部分特异性词汇（出现次数少于总人数的千分之一）
- 根据每个词出现的数量映射到数字特征上
- 训练KNN
#### 复现方式
保证项目结构，root目录下有MyModel目录和personality_dataset。

我们提供了两种复现的方法

- 运行Example.ipynb，实现固定100样本训练集的模型训练，并完成对前一百个样本的预测，此方式仅仅用于了解模型的训练流程和鉴定模型是否可以完成训练。
- 运行main.py，注意完成Config类的创建，源代码已经给出示例，其中需要说明的是，Config的参数，path：数据集目录的路径，valid_fliter:是否删除特异性单词，weight：KNN分类器的投票权重，可为KNN内置的权重函数或者自定义的接受distance作为输入的权重函数，number：加载的训练集和验证集个数，为-1则全部加载，tqdm：是否显示进度条。推荐使用这种方式复现结果，注意的是完整加载整个数据集并验证结果大约需要7小时以上，可酌情加载数据集。

### 子问题2
#### 概述
在这个问题中，我们采用了bert-base-uncase作为预训练模型，通过transforms库将数据集转化为可训练的类型进行训练，读取bert模型中的输出作为特征，使用线性分类器进行分类。
#### 思路
导入数据，利用transforms进行编码，编码完成后利用pythorch完成神经网络的训练。
#### 实施细节
##### 预训练模型的加载
为了避免可能出现的因为上网代理设置问题，导致无法正常从官网下载预训练模型，我们直接在文件中提供了bert-base-uncase的预训练模型的pytorch版，可以直接加载。
#### 复现流程
直接运行在BertModel中提供的Example.ipynb文件，确保数据集路径和模型路径正确即可。
要注意的点是，Bert-base模型参数量庞大，训练过程冗长，建议减少参与训练的数据规模，训练时长仅供参考：100个样本5个循环的训练＋测试大约需要3小时。
### 子问题3
详情见task-1->Report->document.pdf

## Task 2
### 子问题1
#### 概述
对于HPR的难点，在实验报告中有详细提到，请参看task-2->Report->document.pdf
##### 难点一
不同性格之间文本内容构成可能很相似
##### 难点二
分类方法不合理，多种分类可能太细致，让文本上差距很小
### 子问题2
#### 实验流程
- 统计各种性格的数量，从中选择两种完全对立的性格然后比较他们的文本之间的相似度。
- 统计每一个维度的性格的数量，选择J和P这两个样本数量相似的性格，然后仅仅只做二分类，观察分类效果。
#### 复现流程
(如果目录结构有修改需要自行更改代码中的路径，默认预训练模型和数据集在根目录下)
运行Analize.ipynb即可。

## Task 3
### 子问题1
#### 概述
在上一个任务中讨论了MBTI用于文本分类的不太合理的地方，在这一部分，我使用了聚类算法来进行分类的研究，为了能和此前的任务呼应，我依然将样本分为16个类，方便与之前的结果进行比较。
#### 复现流程
在task-3.ipynb中运行代码，直到`train_cluster_id`和`valid_cluster_id`出现，便完成了新的分类标签的设计。
### 子问题2
#### 概述
依托于新的分类标签，重新进行新的实验，借助更小的tiny网络，我可以在本地完成对整个数据集的训练。
#### 复现流程
继续运行task-3.ipynb中的代码，最终会在运行300个训练轮次后终止。注意的是调整`DataFilePath`和`BertModelPath`来匹配自己的数据集与预训练模型位置。或者你也可以直接从网络下载预训练模型。