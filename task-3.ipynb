{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "<center>新的分类标签</center>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFilePath = \"./personality_dataset\"\n",
    "BertModelPath = \"./bert-tiny-uncase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "\n",
    "def data_load(path):\n",
    "    train = datasets.load_from_disk(path + \"/train\")\n",
    "    valid = datasets.load_from_disk(path + \"/valid\")\n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = data_load(DataFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\王蔚昕\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "\n",
    "bert_model = BertModel.from_pretrained(BertModelPath)\n",
    "tokenizer = BertTokenizer.from_pretrained(BertModelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58972/58972 [05:03<00:00, 194.22it/s]\n",
      "100%|██████████| 3104/3104 [00:16<00:00, 192.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "train_data = []\n",
    "for person in tqdm(train):\n",
    "    train_data.append(tokenizer(person[\"content\"].replace(\"|||\", \"[SEP]\"), padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\"))\n",
    "\n",
    "valid_data = []\n",
    "for person in tqdm(valid):\n",
    "    valid_data.append(tokenizer(person[\"content\"].replace(\"|||\", \"[SEP]\"), padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_data):\n",
    "        self.texts = train_data\n",
    "    \n",
    "    def classes(self):\n",
    "        return self.texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def get_batch_texts(self, idx):\n",
    "        return self.texts[idx]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        return batch_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58972/58972 [02:32<00:00, 386.94it/s]\n",
      "100%|██████████| 3104/3104 [00:07<00:00, 405.19it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dataload = torch.utils.data.DataLoader(Dataset(train_data), batch_size=1, worker_init_fn=4)\n",
    "bert_model.to(device)\n",
    "train_feature = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(train_dataload):\n",
    "        torch.cuda.empty_cache()\n",
    "        train_feature.append(bert_model(data[\"input_ids\"].squeeze(1).to(device), data[\"attention_mask\"].to(device))[\"pooler_output\"])\n",
    "        \n",
    "valid_dataload = torch.utils.data.DataLoader(Dataset(valid_data), batch_size=1, worker_init_fn=4)\n",
    "valid_feature = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(valid_data):\n",
    "        torch.cuda.empty_cache()\n",
    "        valid_feature.append(bert_model(data[\"input_ids\"].squeeze(1).to(device), data[\"attention_mask\"].to(device))[\"pooler_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58972/58972 [00:02<00:00, 23491.77it/s]\n",
      "100%|██████████| 3104/3104 [00:00<00:00, 19448.71it/s]\n"
     ]
    }
   ],
   "source": [
    "train_feature_np = [data.cpu().numpy().reshape(128) for data in tqdm(train_feature)]\n",
    "valid_feature_np = [data.cpu().numpy().reshape(128) for data in tqdm(valid_feature)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=16, n_init=\"auto\")\n",
    "train_cluster_id = kmeans.fit_predict(train_feature_np)\n",
    "valid_cluster_id = kmeans.predict(valid_feature_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "<center>依照新的分类标准重新实验</center>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.from_numpy(train_cluster_id)\n",
    "valid_labels = torch.from_numpy(valid_cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Sequential(torch.nn.Linear(128, 256),\n",
    "                                         torch.nn.Dropout(0.5),\n",
    "                                         torch.nn.ReLU(),\n",
    "                                         torch.nn.Linear(256, 512),\n",
    "                                         torch.nn.Dropout(0.5),\n",
    "                                         torch.nn.ReLU(),\n",
    "                                         torch.nn.Linear(512, 256),\n",
    "                                         torch.nn.Dropout(0.5),\n",
    "                                         torch.nn.ReLU(),\n",
    "                                         torch.nn.Linear(256, 128),\n",
    "                                         torch.nn.Dropout(0.5),\n",
    "                                         torch.nn.ReLU(),\n",
    "                                         torch.nn.Linear(128, 16),\n",
    "                                         torch.nn.Softmax(dim=1))\n",
    "        \n",
    "    \n",
    "    def forward(self, pooler_output):\n",
    "        return self.model(pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 300 | Process: 460 / 461 | Train Loss:  2.139 | Train Accuracy:  0.690 | Val Loss:  2.158 | Val Accuracy:  0.692\r"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "try:\n",
    "    model = torch.load(\"last_model.pt\").to(device)\n",
    "except FileNotFoundError:\n",
    "    model = MyModel().to(device)\n",
    "\n",
    "epochs = 300\n",
    "learning_rate = 1e-5\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loader_train_texts = torch.utils.data.DataLoader(Dataset(train_feature_np), batch_size=128)\n",
    "loader_train_labels = torch.utils.data.DataLoader(Dataset(train_labels), batch_size=128)\n",
    "loader_valid_texts = torch.utils.data.DataLoader(Dataset(valid_feature_np), batch_size=128)\n",
    "loader_valid_labels = torch.utils.data.DataLoader(Dataset(valid_labels), batch_size=128)\n",
    "\n",
    "for epoch_num in range(epochs):\n",
    "    \n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "    \n",
    "    for train_input, train_label, i in zip(loader_train_texts, loader_train_labels, range(len(loader_train_labels))):\n",
    "\n",
    "        train_input = train_input.to(device)\n",
    "        train_label = train_label.to(device)\n",
    "        \n",
    "        output = model(train_input)\n",
    "\n",
    "        batch_loss = criterion(output, train_label.to(torch.int64))\n",
    "        total_loss_train = batch_loss.item()\n",
    "        \n",
    "        acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "        total_acc_train += acc\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for val_input, val_label in zip(loader_valid_texts, loader_valid_labels):\n",
    "                \n",
    "                val_input = val_input.to(device)\n",
    "                val_label = val_label.to(device)\n",
    "                output = model(val_input)\n",
    "                \n",
    "                batch_loss = criterion(output, val_label.to(torch.int64))\n",
    "                total_loss_val = batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "        \n",
    "        print(\n",
    "            f\"Epochs: {epoch_num + 1} | Process: {i} / {len(loader_train_labels)}\",\n",
    "            f\"| Train Loss: {total_loss_train: .3f}\",\n",
    "            f\"| Train Accuracy: {total_acc_train / len(train_cluster_id): .3f}\",\n",
    "            f\"| Val Loss: {total_loss_val: .3f}\",\n",
    "            f\"| Val Accuracy: {total_acc_val / len(valid_cluster_id): .3f}\", end=\"\\r\")\n",
    "        torch.save(model, \"last_model.pt\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
