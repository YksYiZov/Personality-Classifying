# Personality-Classifying
The Project of NLP By Xinyu Dai

(本项目已经上传github，[我的仓库](https://github.com/YksYiZov/Personality-Classifying/tree/task-1))
## Task 1
### 子问题1
#### 概述
在这个问题中，我们采用了KNN来进行分类任务，具体的执行思路如下
#### 思路
- 导入文本，进行词数统计工作，分析统计的词数，对数据进行预处理
- 将MBTI人格分类任务分为四个子任务，训练四个KNN分类器，每个KNN完成一个方面的二分类任务
#### 实施细节
##### KNN的训练
- 数据的采集工作我们把全部的训练样本分为两类
- 统计两类样本各个词出现的数量
- 删除部分特异性词汇（出现次数少于总人数的千分之一）
- 根据每个词出现的数量映射到数字特征上
- 训练KNN
#### 复现方式
我们提供了两种复现的方法(如果目录结构有修改需要自行更改代码中的路径，默认预训练模型和数据集在根目录下)

- 运行Example.ipynb，实现固定100样本训练集的模型训练，并完成对前一百个样本的预测，此方式仅仅用于了解模型的训练流程和鉴定模型是否可以完成训练。
- 运行main.py，注意完成Config类的创建，源代码已经给出示例，其中需要说明的是，Config的参数，path：数据集目录的路径，valid_fliter:是否删除特异性单词，weight：KNN分类器的投票权重，可为KNN内置的权重函数或者自定义的接受distance作为输入的权重函数，number：加载的训练集和验证集个数，为-1则全部加载，tqdm：是否显示进度条。推荐使用这种方式复现结果，注意的是完整加载整个数据集并验证结果大约需要7小时以上，可酌情加载数据集。

### 子问题2
#### 概述
在这个问题中，我们采用了bert-base-uncase作为预训练模型，通过transforms库将数据集转化为可训练的类型进行训练，读取bert模型中的输出作为特征，使用线性分类器进行分类。
#### 思路
导入数据，利用transforms进行编码，编码完成后利用pythorch完成神经网络的训练。
#### 实施细节
##### 预训练模型的加载
为了避免可能出现的因为上网代理设置问题，导致无法正常从官网下载预训练模型，我们直接在文件中提供了bert-base-uncase的预训练模型的pytorch版，可以直接加载。
#### 复现流程
直接运行在BertModel中提供的Example.ipynb文件，确保数据集路径和模型路径正确即可。
要注意的点是，Bert-base模型参数量庞大，训练过程冗长，建议减少参与训练的数据规模，训练时长仅供参考：100个样本5个循环的训练＋测试大约需要3小时。
### 子问题3
详情见Report->document.pdf